{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from nnunet.training.model_restore import load_model_and_checkpoint_files\n",
    "import torch\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(np.eye(3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competitions_with_custom_Trainers True\n",
      "BraTS2020 True\n",
      "nnUNetTrainerV2BraTSRegions False\n",
      "nnUNetTrainerV2BraTSRegions_moreDA False\n",
      "MMS True\n",
      "nnUNetTrainerV2_MMS False\n",
      "network_trainer False\n",
      "nnUNetTrainer False\n",
      "nnUNetTrainerCascadeFullRes False\n",
      "nnUNetTrainerV2 False\n",
      "nnUNetTrainerV2_CascadeFullRes False\n",
      "nnUNetTrainerV2_DDP False\n",
      "nnUNetTrainerV2_DP False\n",
      "nnUNetTrainerV2_fp32 False\n",
      "nnUNet_variants True\n",
      "architectural_variants True\n",
      "special initialize for 1x1x1 conv in windowing layer\n",
      "using the following model files:  ['/storage/nnUnet/nnUNet_trained_models/nnUNet/3d_fullres/Task107_CTAV/nnUNetTrainerV2_DP_change_normalization_learn_window__nnUNetPlansv2.1/fold_0/model_best.model']\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = 'Task107_CTAV'\n",
    "\n",
    "MODEL_NAME = 'nnUNetTrainerV2_DP_change_normalization_learn_window__nnUNetPlansv2.1'\n",
    "checkpoint_name='model_best'\n",
    "\n",
    "model = f'/storage/nnUnet/nnUNet_trained_models/nnUNet/3d_fullres/{DATASET_NAME}/{MODEL_NAME}'\n",
    "trainer, all_params = load_model_and_checkpoint_files(\n",
    "        model, folds=[0], mixed_precision=True, checkpoint_name=checkpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_checkpoint_ram(all_params[0], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.conv_blocks_context.0.blocks.0.conv.bias\n",
      "module.conv_blocks_context.0.blocks.0.conv.weight\n",
      "module.conv_blocks_context.0.blocks.1.conv.bias\n",
      "module.conv_blocks_context.0.blocks.1.conv.weight\n",
      "module.conv_blocks_context.0.blocks.1.instnorm.bias\n",
      "module.conv_blocks_context.0.blocks.1.instnorm.weight\n",
      "module.conv_blocks_context.0.blocks.2.conv.bias\n",
      "module.conv_blocks_context.0.blocks.2.conv.weight\n",
      "module.conv_blocks_context.0.blocks.2.instnorm.bias\n",
      "module.conv_blocks_context.0.blocks.2.instnorm.weight\n",
      "module.conv_blocks_context.1.blocks.0.conv.bias\n",
      "module.conv_blocks_context.1.blocks.0.conv.weight\n",
      "module.conv_blocks_context.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_context.1.blocks.1.conv.bias\n",
      "module.conv_blocks_context.1.blocks.1.conv.weight\n",
      "module.conv_blocks_context.1.blocks.1.instnorm.bias\n",
      "module.conv_blocks_context.1.blocks.1.instnorm.weight\n",
      "module.conv_blocks_context.2.blocks.0.conv.bias\n",
      "module.conv_blocks_context.2.blocks.0.conv.weight\n",
      "module.conv_blocks_context.2.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.2.blocks.0.instnorm.weight\n",
      "module.conv_blocks_context.2.blocks.1.conv.bias\n",
      "module.conv_blocks_context.2.blocks.1.conv.weight\n",
      "module.conv_blocks_context.2.blocks.1.instnorm.bias\n",
      "module.conv_blocks_context.2.blocks.1.instnorm.weight\n",
      "module.conv_blocks_context.3.blocks.0.conv.bias\n",
      "module.conv_blocks_context.3.blocks.0.conv.weight\n",
      "module.conv_blocks_context.3.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.3.blocks.0.instnorm.weight\n",
      "module.conv_blocks_context.3.blocks.1.conv.bias\n",
      "module.conv_blocks_context.3.blocks.1.conv.weight\n",
      "module.conv_blocks_context.3.blocks.1.instnorm.bias\n",
      "module.conv_blocks_context.3.blocks.1.instnorm.weight\n",
      "module.conv_blocks_context.4.blocks.0.conv.bias\n",
      "module.conv_blocks_context.4.blocks.0.conv.weight\n",
      "module.conv_blocks_context.4.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.4.blocks.0.instnorm.weight\n",
      "module.conv_blocks_context.4.blocks.1.conv.bias\n",
      "module.conv_blocks_context.4.blocks.1.conv.weight\n",
      "module.conv_blocks_context.4.blocks.1.instnorm.bias\n",
      "module.conv_blocks_context.4.blocks.1.instnorm.weight\n",
      "module.conv_blocks_context.5.0.blocks.0.conv.bias\n",
      "module.conv_blocks_context.5.0.blocks.0.conv.weight\n",
      "module.conv_blocks_context.5.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.5.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_context.5.1.blocks.0.conv.bias\n",
      "module.conv_blocks_context.5.1.blocks.0.conv.weight\n",
      "module.conv_blocks_context.5.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_context.5.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.0.0.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.0.0.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.0.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.0.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.0.1.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.0.1.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.0.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.0.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.1.0.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.1.0.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.1.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.1.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.1.1.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.1.1.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.1.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.1.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.2.0.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.2.0.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.2.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.2.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.2.1.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.2.1.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.2.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.2.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.3.0.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.3.0.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.3.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.3.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.3.1.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.3.1.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.3.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.3.1.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.4.0.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.4.0.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.4.0.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.4.0.blocks.0.instnorm.weight\n",
      "module.conv_blocks_localization.4.1.blocks.0.conv.bias\n",
      "module.conv_blocks_localization.4.1.blocks.0.conv.weight\n",
      "module.conv_blocks_localization.4.1.blocks.0.instnorm.bias\n",
      "module.conv_blocks_localization.4.1.blocks.0.instnorm.weight\n",
      "module.seg_outputs.0.weight\n",
      "module.seg_outputs.1.weight\n",
      "module.seg_outputs.2.weight\n",
      "module.seg_outputs.3.weight\n",
      "module.seg_outputs.4.weight\n",
      "module.tu.0.weight\n",
      "module.tu.1.weight\n",
      "module.tu.2.weight\n",
      "module.tu.3.weight\n",
      "module.tu.4.weight\n"
     ]
    }
   ],
   "source": [
    "state_dict = all_params[0]['state_dict']\n",
    "state_dict_keys = sorted(list(state_dict.keys()))\n",
    "for i in state_dict_keys:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0296226739883423, -0.04535407945513725)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k2 = state_dict['module.conv_blocks_context.0.blocks.0.conv.weight'].item()\n",
    "n2 = state_dict['module.conv_blocks_context.0.blocks.0.conv.bias'].item()\n",
    "k2, n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-156.3082154243652, 1009.1672838201149)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k2, n2 = 1.1737, 0.1038\n",
    "lower_bound = -200\n",
    "upper_bound = 1000\n",
    "\n",
    "k1 = 1/600\n",
    "n1 = -2/3\n",
    "\n",
    "k_final = k1*k2\n",
    "n_final = n1*k2+n2\n",
    "\n",
    "(-1-n_final)/k_final, (1-n_final)/k_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list = [\n",
    "    state_dict['module.conv_blocks_context.0.blocks.0.conv.weight'].numpy().flatten(), \n",
    "]\n",
    "vector_names=['1st_CT_conv', 'first_MR_conv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for k in range(0, 32, 16):\n",
    "    for i in range(8):\n",
    "        idx.append(np.array([k,k+8]) + i)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "tmp_keys = state_dict_keys[:32]\n",
    "_list = [\n",
    "    state_dict[k].numpy().flatten() for k in tmp_keys\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 16, figsize=(17, 7))\n",
    "for enum, ax in enumerate(axes.flatten()):\n",
    "    ax.violinplot([_list[i] for i in idx[enum]])\n",
    "    ax.set_xticks([1, 2], [tmp_keys[i].replace('module.conv_blocks_context.', '') for i in idx[enum]], rotation=90, fontsize=6)\n",
    "fig.tight_layout(w_pad=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_keys = state_dict_keys[32:64]\n",
    "_list = [\n",
    "    state_dict[k].numpy().flatten() for k in tmp_keys\n",
    "]\n",
    "fig, axes = plt.subplots(1, 16, figsize=(17, 7))\n",
    "for enum, ax in enumerate(axes.flatten()):\n",
    "    ax.violinplot([_list[i] for i in idx[enum]])\n",
    "    ax.set_xticks([1, 2], [tmp_keys[i].replace('module.conv_blocks_context.', '') for i in idx[enum]], rotation=90, fontsize=6)\n",
    "fig.tight_layout(w_pad=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = state_dict['module.conv_blocks_context.2.blocks.0.conv.weight'].numpy()\n",
    "print(w.shape)\n",
    "_list = [w[:,:32].flatten(), w[:,32:].flatten()]\n",
    "# _list = [w[:64,:].flatten(), w[64:,:].flatten()]\n",
    "vector_names = ['ct', 'mr']\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.violinplot(_list, showmeans=True, quantiles=[[0.25, 0.75], [0.25, 0.75]])\n",
    "plt.xticks(np.arange(len(_list))+1, vector_names)\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_fp = '/media/medical/projects/head_and_neck/nnUnet/Task208_ONKOI-bothM-curatedFinal-MR-denoise/ct_patch_sitk43.nii.gz'\n",
    "ct_img = sitk.GetArrayFromImage(sitk.ReadImage(ct_fp))\n",
    "mr_img = sitk.GetArrayFromImage(sitk.ReadImage(ct_fp.replace('ct_', 'mr_')))\n",
    "\n",
    "vector_names=['vals_CT', 'vals_MR']\n",
    "_list = [\n",
    "    ct_img.flatten(), mr_img.flatten()\n",
    "]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.boxplot(_list)\n",
    "plt.xticks(np.arange(len(_list))+1, vector_names)\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convw = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_BASE_DIR= f'/media/medical/projects/head_and_neck/nnUnet/{DATASET_NAME}/{MODEL_NAME}'\n",
    "def plot_activations(activation, actv_name, vmin, vmax, modalityblocked, slc = 28, img_base_dir=IMG_BASE_DIR, show=True):\n",
    "    os.makedirs(img_base_dir, exist_ok=True)\n",
    "    actv = activation[actv_name]\n",
    "    num_filters = actv.shape[1]\n",
    "    \n",
    "    ncols_options = 2**np.arange(6)\n",
    "    n_cols_and_rows = ncols_options[np.abs(ncols_options - np.sqrt(num_filters)).argmin()]\n",
    "    n_cols_and_rows = [n_cols_and_rows, num_filters//n_cols_and_rows]\n",
    "    ncols = max(n_cols_and_rows)\n",
    "    nrows = min(n_cols_and_rows)\n",
    "    \n",
    "    fig_k = 1 if num_filters > 100 else 2\n",
    "    figsize=(ncols*fig_k, nrows*fig_k)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    for enum, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(actv[0,enum,slc], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        \n",
    "    plt.suptitle(actv_name + ' ' + str(actv.shape) + f' DISPLAY vmin={vmin}, vmax={vmax}')\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.savefig(join(img_base_dir, f'slc-{slc}_{actv_name}_BLOCKED-{modalityblocked}.png'))\n",
    "    if not show:\n",
    "        plt.close('all')\n",
    "        \n",
    "def get_percentiles(a):\n",
    "    vmin = np.percentile(a.flatten(), 1)\n",
    "    vmax = np.percentile(a.flatten(), 99)\n",
    "    return vmin, vmax\n",
    "\n",
    "def plot_boxplot_activations(activation, actv_names, modalityblocked, img_base_dir=IMG_BASE_DIR, ylim=None):\n",
    "    _list = [activation[a].flatten() for a in actv_names]\n",
    "    ylimstr = ''\n",
    "    if ylim is not None:\n",
    "        _list = [np.clip(i, *ylim) for i in _list]\n",
    "        ylimstr = 'clipped'\n",
    "    plt.figure(figsize=(4, 10))\n",
    "    plt.violinplot(_list)\n",
    "    plt.xticks(np.arange(len(_list))+1, actv_names)\n",
    "    plt.xticks(rotation=90, fontsize=7)\n",
    "    plt.grid()\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(join(img_base_dir, f'{\"--\".join(actv_names)}_BLOCKED-{modalityblocked}_{ylimstr}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 192, 192) (1, 2, 40, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "SLC = 28\n",
    "SHOW=False\n",
    "\n",
    "modalityblocked = 'mr'\n",
    "data = np.stack((ct_img, mr_img*0))[np.newaxis]\n",
    "print(mr_img.shape, data.shape)\n",
    "data_torch = torch.from_numpy(data).float().to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "trainer.network.to(device='cuda:0')\n",
    "trainer.network.eval()\n",
    "\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        a = trainer.network(data_torch)\n",
    "print(a[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.network.conv_blocks_localization[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attr_or_element(base, _list):\n",
    "    for e, l in enumerate(_list):\n",
    "        if isinstance(l, int):\n",
    "            base = base[l]\n",
    "        else:\n",
    "            base = getattr(base, l)\n",
    "        return get_attr_or_element(base, _list[e+1:])\n",
    "    return base\n",
    "get_attr_or_element(trainer.network, ['conv_blocks_context', 0, 'blocks', 0, 'conv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f2b9220c700>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach().cpu().numpy()\n",
    "    return hook\n",
    "# module.conv_blocks_context.0.blocks.0.conv.weight\n",
    "getattr(getattr(getattr(trainer.network, 'conv_blocks_context')[0], \"blocks\")[0], 'conv').register_forward_hook(get_activation('output of 1st conv in the 1st block of CT path'))\n",
    "# trainer.network.conv_blocks_context[0].blocks[0].conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activation = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach().cpu().numpy()\n",
    "    return hook\n",
    "trainer.network.to(device='cuda:0')\n",
    "trainer.network.eval()\n",
    "model = trainer.network\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        trainer.network.conv_blocks_context[0].blocks[0].conv.register_forward_hook(get_activation('output of 1st conv in the 1st block of CT path'))\n",
    "        trainer.network.conv_blocks_context[0].blocks2[0].conv.register_forward_hook(get_activation('output of 1st conv in the 1st block of MR path'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[0].blocks1[1].conv.register_forward_hook(get_activation('output of 2nd conv in the 1st block of CT path'))\n",
    "        # trainer.network.conv_blocks_context[0].blocks2[1].conv.register_forward_hook(get_activation('output of 2nd conv in the 1st block of MR path'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[1].blocks1[0].conv.register_forward_hook(get_activation('output of 1st conv in the 2nd block of CT path'))\n",
    "        # trainer.network.conv_blocks_context[1].blocks2[0].conv.register_forward_hook(get_activation('output of 1st conv in the 2nd block of MR path'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[1].blocks1[1].conv.register_forward_hook(get_activation('output of 2nd conv in the 2nd block of CT path'))\n",
    "        # trainer.network.conv_blocks_context[1].blocks2[1].conv.register_forward_hook(get_activation('output of 2nd conv in the 2nd block of MR path'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[1].blocks1[1].lrelu.register_forward_hook(get_activation('output of 2nd LReLU in the 2nd block of CT path'))\n",
    "        # trainer.network.conv_blocks_context[1].blocks2[1].lrelu.register_forward_hook(get_activation('output of 2nd LReLU in the 2nd block of MR path'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[2].blocks[0].conv.register_forward_hook(get_activation('output of 1st conv in the 3rd block'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[2].blocks[1].lrelu.register_forward_hook(get_activation('output of 2nd LReLU in the 3rd block'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_context[3].blocks[1].lrelu.register_forward_hook(get_activation('output of 2nd LReLU in the 4th block'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_localization[3][1].blocks[0].lrelu.register_forward_hook(get_activation('output of 2nd LReLU in the 4th loc block'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_localization[2][1].blocks[0].lrelu.register_forward_hook(get_activation('output of 3rd loc block'))\n",
    "        \n",
    "        # trainer.network.conv_blocks_localization[3][0].blocks[0].conv.register_forward_hook(get_activation('output of first conv in 4th loc block'))\n",
    "        \n",
    "        # trainer.network.tu[0].register_forward_hook(get_activation('output of transp conv before the 1st loc block'))\n",
    "        \n",
    "        # trainer.network.tu[1].register_forward_hook(get_activation('output of transp conv before the 2nd loc block'))\n",
    "        \n",
    "        # trainer.network.tu[2].register_forward_hook(get_activation('output of transp conv before the 3rd loc block'))\n",
    "        \n",
    "        # trainer.network.tu[3].register_forward_hook(get_activation('output of transp conv before the 4th loc block'))\n",
    "        \n",
    "        # trainer.network.tu[4].register_forward_hook(get_activation('output of transp conv before the 5th loc block'))\n",
    "\n",
    "        output = trainer.network(data_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in activation.items():\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "vector_names=['actv_CT', 'actv_MR']\n",
    "_list = [\n",
    "    activation['output of 2nd LReLU in the 2nd block of CT path'][:,:,SLC].flatten(), \n",
    "    activation['output of 2nd LReLU in the 2nd block of MR path'][:,:,SLC].flatten()\n",
    "]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.violinplot(_list)\n",
    "plt.xticks(np.arange(len(_list))+1, vector_names)\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vmin, vmax = -0.87109375, 0.57080078125\n",
    "\n",
    "plot_activations(activation, 'output of 2nd conv in the 2nd block of CT path', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)\n",
    "plot_activations(activation, 'output of 2nd conv in the 2nd block of MR path', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -0.246826171875, 0.806640625\n",
    "# vmin, vmax = -0.010528564453125, 0.806640625\n",
    "plot_activations(activation, 'output of 2nd LReLU in the 2nd block of CT path', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)\n",
    "plot_activations(activation, 'output of 2nd LReLU in the 2nd block of MR path', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)\n",
    "\n",
    "# vmin, vmax = -0.246826171875, 0.2337646484375\n",
    "plot_activations(activation, 'output of transp conv before the 4th loc block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convw[modalityblocked] = activation['output of 1st conv in the 3rd block']\n",
    "# vector_names=['actv_CT', 'actv_MR']\n",
    "# actvskupna = activation['output of 1st conv in the 3rd block']\n",
    "# actvskupna.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(4, 4))\n",
    "# plt.violinplot([j.flatten() for i, j in convw.items()])\n",
    "# plt.xticks(np.arange(3)+1, [i for i, j in convw.items()])\n",
    "# plt.xticks(rotation=90, fontsize=7)\n",
    "# plt.grid()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig(f'/media/medical/projects/head_and_neck/nnUnet/Task208_ONKOI-bothM-curatedFinal-MR-denoise/activations_joint_after_1st_conv_block3down_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -1.5, 0.96\n",
    "plot_activations(activation, 'output of 1st conv in the 3rd block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plot_boxplot_activations(activation, ['output of 2nd conv in the 2nd block of CT path', 'output of 2nd conv in the 2nd block of MR path'], modalityblocked, img_base_dir=IMG_BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_boxplot_activations(activation, ['output of 2nd LReLU in the 2nd block of CT path', 'output of 2nd LReLU in the 2nd block of MR path'], modalityblocked, img_base_dir=IMG_BASE_DIR)\n",
    "plot_boxplot_activations(activation, ['output of 2nd LReLU in the 2nd block of CT path', 'output of 2nd LReLU in the 2nd block of MR path'], modalityblocked, img_base_dir=IMG_BASE_DIR, ylim=(-0.05, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -0.00824737548828125, 0.3115234375\n",
    "plot_activations(activation, 'output of 2nd LReLU in the 4th block', vmin=vmin, vmax=vmax, slc=SLC//2, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -0.0159149169921875, 3.5\n",
    "plot_activations(activation, 'output of 2nd LReLU in the 4th loc block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = -0.246826171875, 0.2337646484375\n",
    "plot_activations(activation, 'output of transp conv before the 3rd loc block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax=-1.12890625, 1.2265625\n",
    "plot_activations(activation, 'output of transp conv before the 5th loc block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = plt.imread('/media/medical/projects/head_and_neck/nnUnet/Task208_ONKOI-bothM-curatedFinal-MR-denoise/joint_after_1st_conv_block3down_none.png')[:,:,:3].astype(float)\n",
    "# ct0 = plt.imread('/media/medical/projects/head_and_neck/nnUnet/Task208_ONKOI-bothM-curatedFinal-MR-denoise/joint_after_1st_conv_block3down_ct0.png')[:,:,:3].astype(float)\n",
    "# mr0 = plt.imread('/media/medical/projects/head_and_neck/nnUnet/Task208_ONKOI-bothM-curatedFinal-MR-denoise/joint_after_1st_conv_block3down_mr0.png')[:,:,:3].astype(float)\n",
    "# def normalize(img):\n",
    "#     img = img.astype(float)\n",
    "#     img += 1\n",
    "#     img /= 2\n",
    "#     return img\n",
    "    \n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "# axes[0].imshow(normalize(n-ct0), vmin=0, vmax=1)\n",
    "# axes[0].set_title('ALL - CT_zero features')\n",
    "# axes[1].imshow(normalize(n-mr0), vmin=0, vmax=1)\n",
    "# axes[1].set_title('ALL - MR_zero features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_percentiles(activation['output of first conv in 4th loc block'])\n",
    "\n",
    "vmin, vmax=-2.29296875, 0.79150390625\n",
    "plot_activations(activation, 'output of first conv in 4th loc block', vmin=vmin, vmax=vmax, slc=SLC, modalityblocked=modalityblocked, show=SHOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
